{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Glm4vForConditionalGeneration\n",
    "import torch, pathlib\n",
    "\n",
    "MODEL_PATH = \"THUDM/GLM-4.1V-9B-Thinking\"\n",
    "device = \"cuda\"           # o \"cuda:0\"\n",
    "dtype  = torch.bfloat16   # ocupa la mitad\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "model = Glm4vForConditionalGeneration.from_pretrained(MODEL_PATH, torch_dtype=dtype, device_map=\"auto\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph import MessageGraph\n",
    "\n",
    "def glm4v_call(messages, max_tokens=256):\n",
    "    \"\"\"messages = [SystemMessage | HumanMessage | AIMessage …]\"\"\"\n",
    "    # 2.1 convierte a la lista dict{role,content} que espera el template\n",
    "    m = []\n",
    "    for msg in messages:\n",
    "        # content multimodal → lista; texto → str puro\n",
    "        m.append({\"role\": msg.type, \"content\": msg.content})\n",
    "\n",
    "    # 2.2 tokeniza con el chat_template\n",
    "    inputs = processor.apply_chat_template(\n",
    "        m, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # 2.3 genera y decodifica\n",
    "    out = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    reply = processor.decode(out[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                             skip_special_tokens=False)\n",
    "    return reply\n",
    "\n",
    "# --- nodo LLM para el grafo ---\n",
    "def llm_node(state):\n",
    "    state[\"assistant_text\"] = glm4v_call(state[\"prompt\"])\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb232555",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_RE = re.compile(r\"Action:\\s*(\\{.*?\\})\", re.S)\n",
    "\n",
    "def parse_route(state):\n",
    "    txt = state[\"assistant_text\"]\n",
    "    m   = ACTION_RE.search(txt)\n",
    "    if not m:\n",
    "        return END, {\"final_answer\": txt}\n",
    "    state[\"action\"] = json.loads(m.group(1))\n",
    "    return \"call_tool\", state\n",
    "\n",
    "def call_tool(state):\n",
    "    act = state[\"action\"]\n",
    "    name = act[\"action_type\"]\n",
    "    obs  = TOOLS[name].invoke(act | {})     # disparas la tool\n",
    "    state[\"observation\"] = obs\n",
    "    return \"bridge\", state\n",
    "\n",
    "def bridge(state):\n",
    "    # empaqueta observación → mensaje user multimodal\n",
    "    obs = state[\"observation\"]\n",
    "    user_msg = {\"role\": \"user\", \"content\": [obs] if isinstance(obs, dict) else obs}\n",
    "    state[\"prompt\"].extend([\n",
    "        {\"role\":\"assistant\", \"content\": state[\"assistant_text\"]},\n",
    "        user_msg\n",
    "    ])\n",
    "    return \"llm\", state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = MessageGraph()\n",
    "g.add_node(\"llm\",       llm_node)\n",
    "g.add_node(\"parse\",     parse_route)\n",
    "g.add_node(\"call_tool\", call_tool)\n",
    "g.add_node(\"bridge\",    bridge)\n",
    "\n",
    "g.add_edge(\"llm\",   \"parse\")\n",
    "g.add_edge(\"parse\", \"call_tool\")\n",
    "g.add_edge(\"call_tool\", \"bridge\")\n",
    "g.add_edge(\"bridge\", \"llm\")\n",
    "\n",
    "agent = g.compile()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jokernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
